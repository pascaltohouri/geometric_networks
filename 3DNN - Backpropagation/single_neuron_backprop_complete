# Numpy and Pandas
import numpy as np
import pandas as pd
from numpy.ma.core import concatenate

# SciPy
from scipy.spatial.distance import cdist

# Load dataset
data = pd.read_csv(r"2024_10/simple_variational_inference/winequality-red.csv")

# Select only 'citric acid' and 'alcohol' columns as inputs
X = data[['citric acid', 'alcohol']].values            # shape: (N, 2)

# Import quality variables, convert to float64
y_true = data["quality"].astype(np.float64).values     # shape: (N, )

# Define hyper‑parameters
eta  = 0.02
iter = 10_000

# Set the node number: in‑internal‑out, and dimensions. For a single neuron:
N_0, N_1, N_2, D = 2, 1, 1, 3    

# Initialise coordinate matrices (N_l × D)
Z_0 = np.array([[0, 0, 0],
                [0, 0, 1]])                     # inputs
Z_1 = np.array([[0, 0.5, 0.5]])                 # hidden (1 neuron)
Z_2 = np.array([[1, 0, 0.5]])                   # output

def pairwise_euclidean_distances(A, B):
    if not (isinstance(A, np.ndarray) and isinstance(B, np.ndarray)):
        raise TypeError("Both A and B must be NumPy arrays.")
    if A.ndim != 2 or B.ndim != 2:
        raise ValueError("Both A and B must be 2‑D matrices.")
    if A.shape[1] != B.shape[1]:
        raise ValueError(f"Dim mismatch: {A.shape[1]} vs {B.shape[1]}")
    return cdist(A, B, metric='euclidean')          # (rows(A), rows(B))

# Compute pairwise Euclidean distances for weights
w_01 = pairwise_euclidean_distances(Z_0, Z_1)       # (2,1)
w_12 = pairwise_euclidean_distances(Z_1, Z_2)       # (1,1)

# Sigmoid activation and derivative
sigmoid = lambda x: 1 / (1 + np.exp(-x))
sigmoid_derivative = lambda a: a * (1 - a)         

# forward pass
def forward(X, w_01, w_12):
    h      = X @ w_01                               # (N,1)
    a      = sigmoid(h)                             # (N,1)
    y_pred = a @ w_12                               # (N,1)
    return h, a, y_pred

# Mean‑squared‑error loss and derivative
mse_loss         = lambda y_t, y_p: 0.5*np.mean((y_p - y_t)**2)
mse_loss_grad    = lambda y_t, y_p: (y_p - y_t) / y_t.shape[0]

# backward pass 
def backward(X, y_true, h, a, y_pred, w_01, w_12, Z_1):
    # dL/dy_pred
    dL_dy_pred = mse_loss_grad(y_true.reshape(-1,1), y_pred)   # (N,1)

    # gradients wrt weights
    dL_dw12 = a.T @ dL_dy_pred                                 # (1,1)
    dL_da   = dL_dy_pred @ w_12.T                              # (N,1)
    dL_dh   = dL_da * sigmoid_derivative(a)                    # (N,1)
    dL_dw01 = X.T @ dL_dh                                      # (2,1)

    # coordinate‑gradient step
    # \frac{\partialw}{\partialZ} for w_01 and w_12 under Euclidean metric:  \frac{\partial||u‑v||}{\partialv = (v‑u)/||u‑v||}
    eps = 1e‑12
    Delta_01 = Z_1 - Z_0                                       # (2,3) broadcast
    Q_01     = np.linalg.norm(Delta_01, axis=1, keepdims=True) # (2,1)
    dW01_dZ1 = (Delta_01 / (Q_01 + eps)).reshape(N_0, 1, D)    # (2,1,3)

    Delta_12 = Z_2 - Z_1                                       # (1,3)
    Q_12     = np.linalg.norm(Delta_12, axis=1, keepdims=True) # (1,1)
    dW12_dZ1 =‑(Delta_12 / (Q_12 + eps)).reshape(1, 1, D)      # (1,1,3)  (negative since w_12 uses Z_1 as first arg)

    # dL/dZ_1 via weight channels
    dL_dZ1_w01 = np.einsum('ij,ijd->d', dL_dw01.T, dW01_dZ1)   # (3,)
    dL_dZ1_w12 = np.einsum('ij,ijd->d', dL_dw12,  dW12_dZ1)    # (3,)

    # regularisation channel
    # dL_dZ1_reg = np.zeros_like(Z_1).flatten()                # (3,)

    # total coordinate gradient
    dL_dZ1 = dL_dZ1_w01 + dL_dZ1_w12                           # + dL_dZ1_reg

    return dL_dw01, dL_dw12, dL_dZ1.reshape(1,3)

# training loop
for epoch in range(iter):
    h, a, y_pred = forward(X, w_01, w_12)
    dL_dw01, dL_dw12, dL_dZ1 = backward(X, y_true, h, a, y_pred, w_01, w_12, Z_1)

    # gradient‑descent updates
    w_01 -= eta * dL_dw01
    w_12 -= eta * dL_dw12
    Z_1  -= eta * dL_dZ1       # coordinate update

    if epoch % 1000 == 0:
        print(f"Epoch {epoch:>5}: MSE = {mse_loss(y_true, y_pred[:,0]):.4f}")

# -------- test after training --------
print("\nPredictions (first 5 rows):")
print(forward(X, w_01, w_12)[2][:5])
