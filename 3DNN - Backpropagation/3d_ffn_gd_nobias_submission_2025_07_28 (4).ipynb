{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3D SE-FFN: Training\n",
        "A script for repeatedly training the SE-FFN.\n",
        "\n",
        "Due to the model's run time complexity, the 10^4 epoch runs each take ~3-5mins (Colab external runtime). The file's total runtime was 25-55mins with 10 trials (early-stopping dependent)."
      ],
      "metadata": {
        "id": "eA1TFG0g-Fvw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwWUQ5VHYZYF"
      },
      "source": [
        "# Reproducibility Seeding\n",
        "import os, random, numpy as np\n",
        "seed = 42\n",
        "np.random.seed(seed); random.seed(seed)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preamble\n"
      ],
      "metadata": {
        "id": "y5A6rYpeGR77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Packages."
      ],
      "metadata": {
        "id": "dAkF5hS2GatF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SOtKDrHcrzEK"
      },
      "outputs": [],
      "source": [
        "# Maths/data\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import math\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# California Housing Dataset import and preprocessing\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Copying/nesting\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "\n",
        "# Static Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({\n",
        "    # \"font.family\": \"serif\",  # Use a serif font (e.g., Times)\n",
        "    # \"font.serif\": \"Computer Modern Roman\", # Or \"Times New Roman\"\n",
        "    \"font.size\": 10,\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"xtick.labelsize\": 10,\n",
        "    \"ytick.labelsize\": 10,\n",
        "    \"legend.fontsize\": 10,\n",
        "    \"figure.figsize\": (6, 4), # Adjust figure size\n",
        "    \"axes.grid\": True,\n",
        "    \"grid.alpha\": 0.5, # Make grid more subtle\n",
        "    \"lines.linewidth\": 1.5 # Make lines slightly thicker\n",
        "})\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
        "\n",
        "# Save and export data\n",
        "import gzip, pickle, types, pathlib\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWsX4DxI3ObK"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX-OyMdCw9SC"
      },
      "source": [
        "Import and clean data. Normalise data to mean 0 and std dev 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7uHigMAziN8Z"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "data = fetch_california_housing()\n",
        "X_all = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y_df = pd.Series(data.target, name='MedHouseVal')\n",
        "X_df = X_all[['MedInc', 'HouseAge', 'AveRooms', 'AveOccup', 'Latitude', 'Longitude', 'Population', 'AveBedrms']]\n",
        "\n",
        "# DF -> numpy arrays\n",
        "X = X_df.to_numpy()\n",
        "y = y_df.to_numpy()\n",
        "\n",
        "# Prartition into training (70%), validation (15%) and test (15%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Reshape outputs (N_obs, N_features)\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_val = y_val.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "# Scale inputs\n",
        "scaler_X = StandardScaler()\n",
        "X_tr = scaler_X.fit_transform(X_train)\n",
        "X_vl = scaler_X.transform(X_val)\n",
        "X_tt = scaler_X.transform(X_test)\n",
        "\n",
        "# Scale outputs\n",
        "scaler_Y = StandardScaler()\n",
        "y_tr = scaler_Y.fit_transform(y_train)\n",
        "y_vl = scaler_Y.transform(y_val)\n",
        "y_tt = scaler_Y.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l43T471ixlW"
      },
      "source": [
        "# Network Initialisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_xrmCQXtak1"
      },
      "source": [
        "Distribute I-O nodes on parallel $Z_2Z_3$ planes: $Z^{(0)}, Z^{(L-1)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hc8nHO3mrPZ5"
      },
      "outputs": [],
      "source": [
        "def points_on_square(N, side_length=1, x=0):\n",
        "    n = int(math.ceil(math.sqrt(N)))\n",
        "    step = side_length / (n - 1) if n > 1 else side_length / 2\n",
        "\n",
        "    points = []\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            y = i * step\n",
        "            z = j * step\n",
        "            points.append((x, y, z))\n",
        "            if len(points) == N:\n",
        "                return np.array(points)\n",
        "\n",
        "    # Randomly remove excess points\n",
        "    if len(points) > N:\n",
        "        points = np.array(points)\n",
        "        np.random.shuffle(points)\n",
        "        return points[:N]\n",
        "\n",
        "    return np.array(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwHmlyZBPJog"
      },
      "source": [
        "Layer index sorter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E0S7FshhPaty"
      },
      "outputs": [],
      "source": [
        "def l_sort(norm_dict):\n",
        "    l_unique = set()\n",
        "    for (a, b) in norm_dict.keys():\n",
        "        l_unique.add(a)\n",
        "        l_unique.add(b)\n",
        "    return l_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7sSlBWLtrrc"
      },
      "source": [
        "Construct network graph dict:\n",
        "$$\n",
        "\\mathcal{G} = \\bigl\\{\\,\n",
        "0 \\;\\mapsto\\; \\{Z^{(0)}, W^{(0)}, b^{(0)}\\}\\;,\\dots,\\;L-1 \\;\\mapsto\\; \\{Z^{(L-1)}, W^{(L-1)}, b^{(L-1)}\\}\n",
        "\\;,\\bigr\\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v03lulLZjYAW"
      },
      "outputs": [],
      "source": [
        "def build_net(l_list, D=3):\n",
        "    G = {}\n",
        "    for l, N_l in enumerate(l_list):\n",
        "        Gl = {}\n",
        "\n",
        "        if l == 0:\n",
        "            # 1) Fixed input grid, no parameters\n",
        "            N_0 = l_list[0]\n",
        "            Gl[\"Z\"] = points_on_square(N_0, side_length=1, x=0)\n",
        "            Gl[\"W\"] = None\n",
        "            Gl[\"b\"] = None\n",
        "\n",
        "        elif l in range(1, len(l_list)-1):\n",
        "            # 2) Weights from previous to current\n",
        "            Gl[\"Z\"] = np.random.uniform(0.1, 0.9, size=(l_list[l],D))\n",
        "            Gl[\"W\"] = np.ones((N_l, l_list[l-1]))\n",
        "            Gl[\"b\"] = np.ones(N_l)\n",
        "\n",
        "        elif l == len(l_list)-1:\n",
        "\n",
        "            # 3) Fixed output nodes, weights from previous\n",
        "            N_L = l_list[l]\n",
        "            Gl[\"Z\"] = np.array([[1, 0.5, 0.5]]) # distribute_points_on_square(N_L, side_length=1, x=1)\n",
        "            Gl[\"W\"] = np.ones((1, l_list[l-1]))\n",
        "            Gl[\"b\"] = np.zeros(N_l)\n",
        "\n",
        "        G[l] = Gl\n",
        "\n",
        "    return G\n",
        "# Network performs poorly when initialising all nodes symmetrically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr-NN7j_z-8o"
      },
      "source": [
        "# Parameter Conditions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqFarN_Knl7C"
      },
      "source": [
        "## Norms\n",
        "\n",
        "Build node norms dict:\n",
        "$$\n",
        "\\mathcal{Q} = \\bigl\\{\\,\n",
        "(1,\\; 0) \\;\\mapsto\\; ||Z^{(1)}-Z^{(0)}||\\;,\\dots,\\; (L-1,\\; L-2) \\;\\mapsto\\; ||Z^{(L-1)}-Z^{(L-2)}||\n",
        "\\,\\bigr\\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KqYD41tR3nA0"
      },
      "outputs": [],
      "source": [
        "def norm_dict(G):\n",
        "    L = max(G.keys())\n",
        "    Q = {}\n",
        "\n",
        "    # Norm dimension-wise differences\n",
        "    for l in range(L):\n",
        "        Z, Zf = G[l]['Z'], G[l+1]['Z']                 # Zb\n",
        "        Î” = Zf[:, None, :] - Z[None, :, :]             # (n_{l+1}, n_l, D)\n",
        "        Q[(l+1, l)] = np.linalg.norm(Î”, axis=2) + Ïµ    # (n_{l+1}, n_l)\n",
        "\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqAXI7RCWmKi"
      },
      "source": [
        "## Weights\n",
        "Weight condition and derivative:\n",
        "\n",
        "$$\n",
        "w_{n_{l+1, l}, n_{l}} = w_{max} \\cdot tanh(s_w \\cdot (q_{n_{l+1}, n_l}-q_0))\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9DdHWNz8lILL"
      },
      "outputs": [],
      "source": [
        "def compute_w(q, s_w, w_max, q0):\n",
        "    return  w_max * -np.tanh(s_w * (q-q0))\n",
        "\n",
        "def compute_dw(q, s_w, w_max, q0):\n",
        "    # sechÂ²(x) = 1/coshÂ²(x)\n",
        "    sech_sq = 1.0 / np.cosh(s_w * (q - q0))**2\n",
        "    return -w_max * s_w * sech_sq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK9jOjzV6v3P"
      },
      "source": [
        "Build weight dict:\n",
        "\n",
        "$$\n",
        "\\mathcal{W} = \\bigl\\{\\,\n",
        "(1,\\; 0) \\;\\mapsto\\; W^{(1,\\; 0)}\\;,\\dots,\\; (L-1,\\; L-2) \\;\\mapsto\\; W^{(L-1,\\; L-2)}\n",
        "\\,\\bigr\\}\n",
        "$$\n",
        "\n",
        "<br>where:<br>\n",
        "\n",
        "$$\n",
        "W^{(l+1,\\; l)} = \\begin{bmatrix} w^{(l+1)}_{1, 1} \\;\\dots\\; w^{(l+1)}_{1, n_l}\\\\\\vdots \\;\\ddots\\; \\vdots\\\\w^{(l+1)}_{n_{l+1}, 1} \\;\\dots\\; w^{(l+1)}_{n_{l+1}, n_l}\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yZIoa5TlW7Pi"
      },
      "outputs": [],
      "source": [
        "def weight_dict(Q):\n",
        "    ls = l_sort(Q)\n",
        "    W = {}\n",
        "    for l in sorted(ls):\n",
        "      if (l+1, l) in Q:\n",
        "        W[(l+1, l)] = compute_w(Q[(l+1, l)], s_w, w_max, q0)\n",
        "    return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs3xOtP669lB"
      },
      "source": [
        "## Combined Condition (Bias Removed)\n",
        "Apply parameter conditions to net."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jJSq81fcg05F"
      },
      "outputs": [],
      "source": [
        "def update_params(G):\n",
        "    Q = norm_dict(G)\n",
        "    # B = bias_dict(Q)\n",
        "    W = weight_dict(Q)\n",
        "\n",
        "    # G <- new params\n",
        "    ls = l_sort(Q)\n",
        "    for l in range(len(ls) - 1):\n",
        "        N_f = G[l+1][\"Z\"].shape[0]\n",
        "        Gl = {}\n",
        "        Gl[\"Z\"] = G[l+1][\"Z\"]\n",
        "        Gl[\"W\"] = W[(l+1, l)]\n",
        "        Gl[\"b\"] = np.zeros((N_f,))\n",
        "\n",
        "        # Move to next layer\n",
        "        G[l+1] = Gl\n",
        "\n",
        "    return G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txvySsNkjbuU"
      },
      "source": [
        "# Forward Propagation\n",
        "\n",
        "Define forward propagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ajyKiuyYlN1v"
      },
      "outputs": [],
      "source": [
        "def forward(X, G):\n",
        "    cache = {}\n",
        "    A_prev = X\n",
        "    cache[\"A0\"] = X  # store input as A0\n",
        "\n",
        "    for l in sorted(i for i in G if i != 0):\n",
        "        Wl = G[l][\"W\"]                        # (N_(l-1), N_l)\n",
        "        bl = G[l][\"b\"]                        # (1, N_l)\n",
        "        Al = np.dot(A_prev, Wl.T) + bl        # (N_l, N)\n",
        "        cache[f\"A{l}\"] = Al\n",
        "        A_prev = Al\n",
        "\n",
        "    AL = A_prev\n",
        "    return AL, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKsHqpPCRUny"
      },
      "source": [
        "# Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY04FbCMZAe3"
      },
      "source": [
        "## Loss and Regulariser\n",
        "Regularised loss:\n",
        "$\\mathcal{L}_{reg} = \\frac{1}{2} \\Sigma_{l} Î£_{n_{l+1},n_l} [s_{reg} Â· (q^{(l+1)}_{n_{l+1}, n_l} - q_0)]^2$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a3_tUIOhZAEN"
      },
      "outputs": [],
      "source": [
        "def loss_reg(G):\n",
        "    l_reg = 0.0\n",
        "    Q = norm_dict(G)\n",
        "    for q in Q.values():\n",
        "        l_reg += np.sum((s_reg * (q - q0)) ** 2)\n",
        "    return l_reg\n",
        "\n",
        "def loss_pred(Y, AL):\n",
        "    NL, N = Y.shape\n",
        "    return (1 / (2 * NL * N)) * np.sum((Y - AL)**2)\n",
        "\n",
        "def loss(Y, AL, G):\n",
        "    return loss_pred(Y, AL) + loss_reg(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1caxGSWZbBp"
      },
      "source": [
        "## Parameter Step\n",
        "Define the conventional backpropagation algorithm.\n",
        "\n",
        "$$\\frac{d\\mathcal{L}}{d\\theta} = \\bigl\\{\\,\n",
        "(1,\\; 0) \\;\\mapsto\\; \\{\\frac{d\\mathcal{L}}{dW^{(1,\\; 0)}}, \\frac{d\\mathcal{L}}{db^{(1)}}\\}\\;,\\dots,\\; (L-1,\\; L-2) \\;\\mapsto\\; \\{\\frac{d\\mathcal{L}}{dW^{(L-1,\\; L-2)}}, \\frac{d\\mathcal{L}}{db^{(L-1)}}\\}\n",
        "\\,\\bigr\\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qbZuQBvrjBm0"
      },
      "outputs": [],
      "source": [
        "def back_Î¸(Y, cache, G):\n",
        "    dÎ¸ = {}\n",
        "    L = len(G)-1\n",
        "    N = Y.shape[0]\n",
        "    NL = Y.shape[1]\n",
        "    AL = cache[f\"A{L}\"]\n",
        "\n",
        "    # dLdy_pred\n",
        "    dAL = ( 1 / (NL * N) ) * (AL - Y)\n",
        "\n",
        "    # Backpropagate layer-wise\n",
        "    dA = dAL\n",
        "    for l in reversed(range(1, L + 1)):\n",
        "        Al = cache[f\"A{l}\"]\n",
        "        Wl = G[l][\"W\"]\n",
        "\n",
        "        # Compute gradients\n",
        "        dW = np.dot(dA.T, cache[f\"A{l-1}\"])\n",
        "        db = np.sum(dA, axis=0)\n",
        "        dA = np.dot(dA, Wl)\n",
        "\n",
        "        # Store gradients\n",
        "        dÎ¸[f\"dW{l}\"] = dW\n",
        "        dÎ¸[f\"db{l}\"] = db\n",
        "\n",
        "    return dÎ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNMnUHwR3MLF"
      },
      "source": [
        "## Coordinate Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQajUc5IQdC"
      },
      "source": [
        "### Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XM6vOtq7yHZ"
      },
      "source": [
        "Build weight-coordinate derivatives $\\frac{\\partial W}{\\partial Z^{(l+1)}}$ and $\\frac{\\partial W}{\\partial Z^{(l)}}$:\n",
        "\n",
        "<br>\n",
        "\n",
        "Forward:\n",
        "$$\n",
        "\\underbrace{\\frac{\\partial W^{(l+1)}}{\\partial Z^{(l+1)}}}_{(N_{l+1},\\; N_l,\\; D)}\\ = \\begin{bmatrix}\\frac{\\partial w_{1, 1}}{\\partial z_{1,d}}\\; \\dots \\; \\frac{\\partial w_{1,n_l}}{\\partial z_{1,d}} \\\\\\vdots \\; \\ddots \\; \\vdots\\\\ \\frac{\\partial w_{n_{l+1}, 1}}{\\partial z_{n_{l+1},d}}\\; \\dots \\; \\frac{\\partial w_{n_{l+1}, n_l}}{\\partial z_{n_{l+1}, d}}\\end{bmatrix}^{d=1} \\dots \\quad \\begin{bmatrix}\\frac{\\partial w_{1, 1}}{\\partial z_{1,d}}\\; \\dots \\; \\frac{\\partial w_{1,n_l}}{\\partial z_{1,d}} \\\\\\vdots \\; \\ddots \\; \\vdots\\\\ \\frac{\\partial w_{n_{l+1}, 1}}{\\partial z_{n_{l+1}l,d}}\\; \\dots \\; \\frac{\\partial w_{n_{l+1}, n_l}}{\\partial z_{n_{l+1}, d}}\\end{bmatrix}^{d=D}$$\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Backward:\n",
        "$$\n",
        "\\underbrace{\\frac{\\partial W^{(l+1)}}{\\partial Z^{(l)}}}_{(N_{l+1},\\; N_l,\\; D)}\\ = \\begin{bmatrix}\\frac{\\partial w_{1, 1}}{\\partial z_{1,d}}\\; \\dots \\; \\frac{\\partial w_{1,n_l}}{\\partial z_{n_l,d}} \\\\\\vdots \\; \\ddots \\; \\vdots\\\\ \\frac{\\partial w_{n_{l+1}, 1}}{\\partial z_{1,d}}\\; \\dots \\; \\frac{\\partial w_{n_{l+1}, n_l}}{\\partial z_{n_l, d}}\\end{bmatrix}^{d=1} \\dots \\quad \\begin{bmatrix}\\frac{\\partial w_{1, 1}}{\\partial z_{1,d}}\\; \\dots \\; \\frac{\\partial w_{1,n_l}}{\\partial z_{n_l,d}} \\\\\\vdots \\; \\ddots \\; \\vdots\\\\ \\frac{\\partial w_{n_{l+1}, 1}}{\\partial z_{1,d}}\\; \\dots \\; \\frac{\\partial w_{n_{l+1}, n_l}}{\\partial z_{n_l, d}}\\end{bmatrix}^{d=D}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hQHz7TkHx73c"
      },
      "outputs": [],
      "source": [
        "def get_dWdZ(Zf, Z, Ïµ):\n",
        "    Î” = Zf[:, np.newaxis, :] - Z[np.newaxis,:, :]    # (N_{l+1}, N_l, D)\n",
        "    Ql = np.sqrt(np.sum(Î”**2, axis=-1) + Ïµ)          # (N_{l+1}, N_l)\n",
        "\n",
        "    dWdQl = compute_dw(Ql, s_w, w_max, q0)\n",
        "\n",
        "    # Compute derivatives w.r.t. coordinates\n",
        "    s = (dWdQl / Ql)[..., np.newaxis]       # (N_{l+1}, N_l, 1)\n",
        "    dWdZ = -s * Î”                           # (N_{l+1}, N_l, D)\n",
        "    dWdZf = -dWdZ\n",
        "\n",
        "    return dWdZ, dWdZf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y__YZdk5EIEk"
      },
      "source": [
        "Compile loss-coordinates derivative dict. via weights $\\frac{\\partial \\mathcal{L}}{\\partial Z}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\small{\n",
        "\\left(\\frac{\\partial \\mathcal{L}}{\\partial Z^{(l)}}\\right)_{n_l, d} = \\sum_{n_{l+1}}\\left[\\left(\\frac{\\partial \\mathcal{L}}{\\partial W^{(l+1)}}\\right)_{n_{l+1}, n_l} \\cdot \\left(\\frac{\\partial W^{(l+1)}}{\\partial Z^{(l)}}\\right)_{n_{l+1}, n_l, d}\\right]\n",
        "+\n",
        "\\sum_{n_{l-1}}\\left[\\left(\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}\\right)_{n_l, n_{l-1}} \\cdot \\left(\\frac{\\partial W^{(l)}}{\\partial Z^{(l)}}\\right)_{n_{l+1}, n_l, d}\\right]\n",
        "}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial Z^{(l)}} = \\begin{bmatrix}\\frac{\\partial \\mathcal{L}}{\\partial z_{1, 1}} & \\dots & \\frac{\\partial \\mathcal{L}}{\\partial z_{1, D}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial \\mathcal{L}}{\\partial z_{N_l, 1}} & \\dots & \\frac{\\partial \\mathcal{L}}{\\partial z_{N_l, D}}\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DKOoO1vNVj1M"
      },
      "outputs": [],
      "source": [
        "def back_W(dÎ¸, G):\n",
        "    dWdZ = {}\n",
        "    dLdZ_W = {}\n",
        "    L = len(G) - 1\n",
        "\n",
        "    for l in range(1, L):\n",
        "        Z = G[l][\"Z\"]           # (N_l, D)\n",
        "        Zf = G[l+1][\"Z\"]        # (N_{l+1}, D)\n",
        "        Zb = G[l-1][\"Z\"]        # (N_{l-1}, D)\n",
        "\n",
        "        # Forward differences: current â†’ next\n",
        "        dWdZf, _ = get_dWdZ(Zf, Z, Ïµ)\n",
        "\n",
        "        # Backward differences: current â†’ previous\n",
        "        _, dWdZb = get_dWdZ(Z, Zb, Ïµ)\n",
        "\n",
        "        # Store dWdZ\n",
        "        dWdZ[l] = {\n",
        "            'forward': dWdZf,\n",
        "            'backward': dWdZb\n",
        "        }\n",
        "\n",
        "        # Chain dLdW and dWdZ (forward)\n",
        "        dLdWf = dÎ¸[f\"dW{l+1}\"]\n",
        "        dLdZf = np.einsum('ab,abd->bd', dLdWf, dWdZ[l]['forward'])\n",
        "\n",
        "        # Chain dLdW and dWdZ (backward)\n",
        "        dLdWb = dÎ¸[f\"dW{l}\"]\n",
        "        dLdZb = np.einsum('bc,bcd->bd', dLdWb, dWdZ[l]['backward'])\n",
        "\n",
        "        dLdZ_W[l] = dLdZf + dLdZb\n",
        "\n",
        "    return dLdZ_W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1gLTA0liLsZ"
      },
      "source": [
        "### Regulariser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N3y_vpT1q4_"
      },
      "source": [
        "Regularisation derivative:  $\\frac{\\partial L_{reg}}{\\partial q_{n_{l+1},n_l}} = s_{reg}^2 \\cdot (q_{n_{l+1},n_l}-q_0)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6k_5IlJo2FVt"
      },
      "outputs": [],
      "source": [
        "# Compute a dict like norms containing dL_reg/dQ_{ji}\n",
        "def get_dLdQ_r(Q):\n",
        "    s_reg2      = s_reg * s_reg\n",
        "    dLdQ  = {}\n",
        "    for key, q in Q.items():\n",
        "        dLdQ[key] = s_reg2 * (q - q0)    # (N_{l+1}, N_L)\n",
        "    return dLdQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcF95w7t3jVN"
      },
      "source": [
        "Get loss-coordinate derivatives via regulariser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0LHnPxygiMK7"
      },
      "outputs": [],
      "source": [
        "def back_r(G, Q):\n",
        "\n",
        "    # Count Layers, calculate total distance, intialise grad dicts\n",
        "    L = max(G.keys())\n",
        "    dLdZ_r = {l: np.zeros_like(G[l]['Z']) for l in range(L + 1)}\n",
        "    dLdQ_r = get_dLdQ_r(Q)\n",
        "\n",
        "    # Loop over layers, calculate loss-coord grads (via reg)\n",
        "    for l in range(L):\n",
        "        Z, Zf = G[l]['Z'], G[l+1]['Z']\n",
        "        Ql    = Q[(l+1, l)]                       # (n_{l+1}, n_l)\n",
        "        Î”    = Zf[:, None, :] - Z[None, :, :]     # (n_{l+1}, n_l, D)\n",
        "\n",
        "        # chain: dL/dZ = dL/dQl Â· dQl/dZ\n",
        "        grad = dLdQ_r[(l+1, l)][:, :, None] * Î” / Ql[:, :, None]\n",
        "        dLdZ_r[l+1] += np.sum(grad, axis=1)  # l+1\n",
        "        dLdZ_r[l]   -= np.sum(grad, axis=0)  # l\n",
        "\n",
        "    return dLdZ_r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkbrjpCEkfkX"
      },
      "source": [
        "# Update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrqE6JCEHTgJ"
      },
      "source": [
        "## Optimiser\n",
        "AdaM for efficient solution exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8-9ILAg2HuCn"
      },
      "outputs": [],
      "source": [
        "def adam(Z, dLdZ, AS, lr=1e-3, r1=0.9, r2=0.999):\n",
        "    m, v, t = AS\n",
        "    t += 1\n",
        "\n",
        "    # 1st & 2nd moment updates\n",
        "    m = r1 * m + (1 - r1) * dLdZ\n",
        "    v = r2 * v + (1 - r2) * (dLdZ * dLdZ)\n",
        "\n",
        "    # Bias correction\n",
        "    m_hat = m / (1 - r1 ** t)\n",
        "    v_hat = v / (1 - r2 ** t)\n",
        "\n",
        "    # Coord update\n",
        "    Z = Z - lr * m_hat / (np.sqrt(v_hat) + Ïµ)\n",
        "\n",
        "    return Z, (m, v, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odAiBczDHuy-"
      },
      "source": [
        "## Gradient Descent\n",
        "Define gradient descent loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eUoMvxeYeD94"
      },
      "outputs": [],
      "source": [
        "from logging import addLevelName\n",
        "def GD(G, eps, its, lr):\n",
        "    # Nested dict (epoch -> iter -> l -> Z)\n",
        "    hy = defaultdict(lambda: defaultdict(dict))\n",
        "    hy_tr = []\n",
        "    hy_vl = []\n",
        "\n",
        "    # Track validation loss for early stopping\n",
        "    l_pr_vlg = float('inf')\n",
        "    ep_unimproved = 0\n",
        "    Gg = None\n",
        "\n",
        "    AS = {\n",
        "    l: {\n",
        "        \"m\": np.zeros_like(G[l][\"Z\"]),\n",
        "        \"v\": np.zeros_like(G[l][\"Z\"]),\n",
        "        \"t\": 0\n",
        "    }\n",
        "    for l in range(1, len(G)-1)\n",
        "    }\n",
        "\n",
        "    # Loop over epochs and iterations\n",
        "    for ep in range(eps):\n",
        "        for it in range(its):\n",
        "\n",
        "            # 1-2) Forward pass, backprop\n",
        "            y_pr, cache = forward(X_tr, G)\n",
        "            dÎ¸ = back_Î¸(y_tr, cache, G)\n",
        "\n",
        "            # 3) Coords backprop (W,b,L_reg)\n",
        "            Q = norm_dict(G)\n",
        "            dLdZ_W = back_W(dÎ¸, G)\n",
        "            dLdZ_r = back_r(G, Q)\n",
        "\n",
        "            # 4) Sum coord grads. Update coords\n",
        "            for l in range(1, len(G)-1):\n",
        "                dLdZ = dLdZ_W.get(l, 0) + dLdZ_r.get(l, 0)\n",
        "\n",
        "                # get AdaM states, apply update\n",
        "                m, v, t   = (AS[l][\"m\"], AS[l][\"v\"], AS[l][\"t\"])\n",
        "                G[l][\"Z\"], (m, v, t) = adam(G[l][\"Z\"], dLdZ, (m, v, t), lr=lr, r1=0.9, r2=0.999)\n",
        "                AS[l].update({\"m\": m, \"v\": v, \"t\": t})\n",
        "\n",
        "            # 5) Update Params\n",
        "            G = update_params(G)\n",
        "\n",
        "            # 6) Record layer coords.\n",
        "            for l in range(0, len(G)):\n",
        "              hy[ep][it][l] = G[l][\"Z\"].copy()\n",
        "\n",
        "            # 7) Compute training error.\n",
        "            l_tr = loss(y_tr, y_pr, G)\n",
        "            l_pr_tr = loss_pred(y_tr, y_pr)\n",
        "            hy_tr.append(l_tr)\n",
        "\n",
        "            # 8) Validation\n",
        "            y_vlpr, _ = forward(X_vl, G)\n",
        "            l_pr_vl = loss_pred(y_vl, y_vlpr)\n",
        "            l_vl = loss(y_vl, y_vlpr, G)\n",
        "            hy_vl.append(l_vl)\n",
        "            #print(f\"Ep. {ep+1}, It. {it+1}\")\n",
        "            #print(f\"Ep. {ep+1}, It. {it+1}: Loss_tr = {l_tr}, Loss_Vl = {l_vl}, Loss_pr_tr = {l_pr_tr},  Loss_pr_vl = {l_pr_vl}\")\n",
        "\n",
        "        # Validation stop check\n",
        "        if l_pr_vl < l_pr_vlg:\n",
        "                l_pr_vlg = l_pr_vl\n",
        "                Gg = deepcopy(G)\n",
        "                ep_unimproved = 0\n",
        "        else:\n",
        "                ep_unimproved += 1\n",
        "        \"\"\"\n",
        "        Optional: early stopping\n",
        "        if ep_unimproved >= 100:\n",
        "            print(\"Early stopping condition met.\")\n",
        "            #break\n",
        "        \"\"\"\n",
        "    \"\"\"\n",
        "    # Plot the loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(len(hy_tr)), hy_tr, label='Training Loss')\n",
        "    plt.plot(range(len(hy_vl)), hy_vl, label='Validation Loss')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Updates')\n",
        "    plt.ylabel('(Log) Loss')\n",
        "    plt.title('Loss vs Updates')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \"\"\"\n",
        "\n",
        "    return Gg, hy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-XShLUdVbZy"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "22zaRbbnzLxv"
      },
      "outputs": [],
      "source": [
        "Ïµ = 1e-8\n",
        "\n",
        "# Weight hyper-parameters\n",
        "q0 = np.sqrt(3)/4 #1/4 works but central clumping occurs\n",
        "w_max = 2\n",
        "w_sat =  0.99   # saturation\n",
        "q_sat = np.sqrt(3)/4\n",
        "\n",
        "# The weight function saturates 100*w_sat (%) at d_s (units) about d_0\n",
        "s_w = np.arctanh(w_sat) / q_sat\n",
        "\n",
        "# Net shape\n",
        "N = X_tr.shape[0]\n",
        "N_0 = X_tr.shape[1]\n",
        "N_L = y_tr.shape[1]\n",
        "l_list = [N_0, 32, 8, 2, N_L]\n",
        "\n",
        "# Initialise net\n",
        "G = build_net(l_list)\n",
        "G = update_params(G)\n",
        "\n",
        "# Run backpropagation\n",
        "s_reg = 1 #0.75\n",
        "\n",
        "basic_run = False\n",
        "if basic_run==True:\n",
        "  G, hy = GD(G=G, eps=5000, its=1, lr=0.04)\n",
        "  y_pr = forward(X_tt, G)[0]\n",
        "  l_pr = loss_pred(y_tt, y_pr)\n",
        "  l_reg = loss_reg(G)\n",
        "  print(f\"Test Pred Loss: {l_pr}.\\n\\nTest Reg. Loss:{l_reg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment (SE-FFN)"
      ],
      "metadata": {
        "id": "efZAqwgT8RRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute multiple GD runs with different parameter settings."
      ],
      "metadata": {
        "id": "NT4J3CKa8dq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle, copy\n",
        "import numpy as np\n",
        "import time, pathlib\n",
        "\n",
        "\n",
        "repeats = 10\n",
        "# Define settings (S) for testing\n",
        "S = [\n",
        "    {\"lr\": 0.04, \"eps\": 100, \"its\": 1, \"name\": \"lr0.04_eps100\"},\n",
        "    {\"lr\": 0.04, \"eps\": 1000, \"its\": 1, \"name\": \"lr0.04_eps1000\"},\n",
        "    {\"lr\": 0.04, \"eps\": 10000, \"its\": 1, \"name\": \"lr0.04_eps10000\"}\n",
        "]\n",
        "\n",
        "best_overall = {\n",
        "    \"val_loss\": np.inf,\n",
        "    \"setting\": None,\n",
        "    \"run_idx\": None,\n",
        "    \"G_best\": None,\n",
        "    \"history\": None\n",
        "}\n",
        "\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "\n",
        "# Set output spacing\n",
        "row_fmt = (\n",
        "    \"  run {run_idx:>2d}/{repeats:<2d} | \"\n",
        "    \"train {tr:>9.4f} | \"\n",
        "    \"val {vl:>9.4f} | \"\n",
        "    \"test {tt:>9.4f} | \"\n",
        "    \"{rt:>7.2f}s\"\n",
        ")\n",
        "summary_fmt = \"  {label:<10} Î¼: {mean:>10.6f}, Ïƒ: {std:>10.6f}\"\n",
        "\n",
        "\n",
        "save_path = pathlib.Path(\"artifacts\")\n",
        "save_path.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Starting multiple SEâ€‘FFN runsâ€¦\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for s_idx, s in enumerate(S):\n",
        "    s_name = s[\"name\"]\n",
        "    eps = s[\"eps\"]\n",
        "    its = s[\"its\"]\n",
        "    lr = s[\"lr\"]\n",
        "\n",
        "    # Sub-title line\n",
        "    print(f\"\\nSetting {s_idx+1}: {s['name']}  (lr={s['lr']}, eps={s['eps']}, its={s['its']})\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    s_results = {\"G_best\": [], \"train_loss\": [], \"val_loss\": [], \"test_loss\": []}\n",
        "\n",
        "    for r in range(repeats):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Initialize network and run optimiser\n",
        "        G_new = build_net(l_list)\n",
        "        G_new = update_params(G_new)\n",
        "        G_best, hy = GD(G_new, eps=s[\"eps\"], its=s[\"its\"], lr=s[\"lr\"])\n",
        "\n",
        "        # Calculate final losses with the best G\n",
        "        y_tr_pred, _ = forward(X_tr, G_best)\n",
        "        y_vl_pred, _ = forward(X_vl, G_best)\n",
        "        y_tt_pred, _ = forward(X_tt, G_best)\n",
        "\n",
        "        train_loss = loss_pred(y_tr, y_tr_pred)\n",
        "        val_loss   = loss_pred(y_vl, y_vl_pred)\n",
        "        test_loss  = loss_pred(y_tt, y_tt_pred)\n",
        "\n",
        "        # Store results\n",
        "        s_results[\"G_best\"].append(G_best)\n",
        "        s_results[\"train_loss\"].append(train_loss)\n",
        "        s_results[\"val_loss\"].append(val_loss)\n",
        "        s_results[\"test_loss\"].append(test_loss)\n",
        "\n",
        "        # Global best cache update\n",
        "        if val_loss < best_overall[\"val_loss\"]:\n",
        "            best_overall.update(\n",
        "                val_loss = val_loss,\n",
        "                setting  = s[\"name\"],\n",
        "                run_idx  = r + 1,\n",
        "                G_best   = copy.deepcopy(G_best),   # deepâ€‘copy to freeze weights\n",
        "                history  = copy.deepcopy(hy)        # loss trajectory\n",
        "            )\n",
        "\n",
        "            # persist immediately so interrupted run retains the artefact\n",
        "            # with open(save_path / \"best_snapshot.pkl\", \"wb\") as f:\n",
        "                # pickle.dump(best_overall, f)\n",
        "\n",
        "            print(f\"\\n ***New optimal model obtained***  Val={val_loss:.6f}  \"\n",
        "                  f\"[{s['name']} | run {r+1}] (checkpoint saved)\\n\")\n",
        "\n",
        "        # Results output line\n",
        "        print(row_fmt.format(run_idx=r+1, repeats=repeats,\n",
        "                             tr=train_loss, vl=val_loss,\n",
        "                             tt=test_loss, rt=time.time()-t0))\n",
        "\n",
        "    all_results[s[\"name\"]] = s_results\n",
        "\n",
        "    # Print summary statistics for current setting\n",
        "    train_losses = s_results[\"train_loss\"]\n",
        "    val_losses = s_results[\"val_loss\"]\n",
        "    test_losses = s_results[\"test_loss\"]\n",
        "\n",
        "    print(f\"\\nSummary for {s_name}:\")\n",
        "    print(summary_fmt.format(label=\"Train Loss\", mean=np.mean(train_losses), std=np.std(train_losses)))\n",
        "    print(summary_fmt.format(label=\"Val Loss\",   mean=np.mean(val_losses),   std=np.std(val_losses)))\n",
        "    print(summary_fmt.format(label=\"Test Loss\",  mean=np.mean(test_losses),  std=np.std(test_losses)))\n",
        "\n",
        "# Comprehensive Results Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL RUNS COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nBEST MODEL OVERALL\")\n",
        "print(\"-\"*40)\n",
        "print(f\"Setting : {best_overall['setting']}\")\n",
        "print(f\"Run     : {best_overall['run_idx']}/{repeats}\")\n",
        "print(f\"Val loss: {best_overall['val_loss']:.6f}\")\n",
        "print(f\"Saved   : {save_path/'best_snapshot.pkl'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQna5JeJTc8K",
        "outputId": "3f55214e-b021-441d-8753-78f988b63fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting multiple SEâ€‘FFN runsâ€¦\n",
            "============================================================\n",
            "\n",
            "Setting 1: lr0.04_eps100  (lr=0.04, eps=100, its=1)\n",
            "----------------------------------------\n",
            "\n",
            " ***New optimal model obtained***  Val=19.815297  [lr0.04_eps100 | run 1] (checkpoint saved)\n",
            "\n",
            "  run  1/10 | train   20.8566 | val   19.8153 | test   29.0266 |    6.83s\n",
            "\n",
            " ***New optimal model obtained***  Val=4.674290  [lr0.04_eps100 | run 2] (checkpoint saved)\n",
            "\n",
            "  run  2/10 | train    5.5018 | val    4.6743 | test    5.1793 |    8.53s\n",
            "  run  3/10 | train   10.0038 | val    7.9407 | test    9.4946 |    2.22s\n",
            "  run  4/10 | train   13.2707 | val   11.4931 | test   14.8666 |    1.47s\n",
            "\n",
            " ***New optimal model obtained***  Val=1.082112  [lr0.04_eps100 | run 5] (checkpoint saved)\n",
            "\n",
            "  run  5/10 | train    2.5722 | val    1.0821 | test    1.0225 |    1.45s\n",
            "  run  6/10 | train   22.1990 | val   23.0274 | test   23.7609 |    1.47s\n",
            "  run  7/10 | train   22.5658 | val   17.4745 | test   27.6440 |    2.48s\n",
            "\n",
            " ***New optimal model obtained***  Val=0.878040  [lr0.04_eps100 | run 8] (checkpoint saved)\n",
            "\n",
            "  run  8/10 | train    1.0315 | val    0.8780 | test    1.2444 |    3.81s\n",
            "  run  9/10 | train   58.6991 | val   32.7249 | test   38.3405 |    1.74s\n",
            "  run 10/10 | train    7.4003 | val    6.3525 | test    6.6016 |    1.50s\n",
            "\n",
            "Summary for lr0.04_eps100:\n",
            "  Train Loss Î¼:  16.410076, Ïƒ:  15.995577\n",
            "  Val Loss   Î¼:  12.546276, Ïƒ:   9.921602\n",
            "  Test Loss  Î¼:  15.718118, Ïƒ:  12.469503\n",
            "\n",
            "Setting 2: lr0.04_eps1000  (lr=0.04, eps=1000, its=1)\n",
            "----------------------------------------\n",
            "\n",
            " ***New optimal model obtained***  Val=0.757711  [lr0.04_eps1000 | run 1] (checkpoint saved)\n",
            "\n",
            "  run  1/10 | train    0.7903 | val    0.7577 | test    1.0896 |   17.51s\n",
            "\n",
            " ***New optimal model obtained***  Val=0.386097  [lr0.04_eps1000 | run 2] (checkpoint saved)\n",
            "\n",
            "  run  2/10 | train    0.4396 | val    0.3861 | test    0.5400 |   19.75s\n",
            "  run  3/10 | train    0.4612 | val    0.4212 | test    0.4626 |   17.81s\n",
            "\n",
            " ***New optimal model obtained***  Val=0.223646  [lr0.04_eps1000 | run 4] (checkpoint saved)\n",
            "\n",
            "  run  4/10 | train    0.2233 | val    0.2236 | test    0.2053 |   17.98s\n",
            "  run  5/10 | train    1.3949 | val    1.2500 | test    1.3563 |   21.72s\n",
            "  run  6/10 | train    0.4511 | val    0.4114 | test    0.4078 |   16.72s\n",
            "  run  7/10 | train    1.0751 | val    1.0576 | test    0.9365 |   20.78s\n",
            "  run  8/10 | train    1.0146 | val    0.9682 | test    0.9248 |   17.27s\n",
            "  run  9/10 | train    0.8728 | val    0.5467 | test    0.5559 |   18.21s\n",
            "  run 10/10 | train    1.2119 | val    1.1557 | test    1.2939 |   21.05s\n",
            "\n",
            "Summary for lr0.04_eps1000:\n",
            "  Train Loss Î¼:   0.793493, Ïƒ:   0.367330\n",
            "  Val Loss   Î¼:   0.717807, Ïƒ:   0.349293\n",
            "  Test Loss  Î¼:   0.777269, Ïƒ:   0.376153\n",
            "\n",
            "Setting 3: lr0.04_eps10000  (lr=0.04, eps=10000, its=1)\n",
            "----------------------------------------\n",
            "  run  1/10 | train    0.2826 | val    0.2440 | test    0.2319 |  187.62s\n",
            "\n",
            " ***New optimal model obtained***  Val=0.203062  [lr0.04_eps10000 | run 2] (checkpoint saved)\n",
            "\n",
            "  run  2/10 | train    0.1970 | val    0.2031 | test    0.1973 |  188.58s\n",
            "\n",
            " ***New optimal model obtained***  Val=0.201839  [lr0.04_eps10000 | run 3] (checkpoint saved)\n",
            "\n",
            "  run  3/10 | train    0.1959 | val    0.2018 | test    0.1914 |  193.28s\n",
            "  run  4/10 | train    0.6930 | val    0.5836 | test    0.9261 |  204.68s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save/export the optimal graph and update history."
      ],
      "metadata": {
        "id": "Rwj-uZwcY4pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove callable entries from the history file\n",
        "def strip_callables(obj):\n",
        "    if callable(obj):\n",
        "        return None\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: strip_callables(v) for k, v in obj.items()\n",
        "                if not callable(v)}\n",
        "    if isinstance(obj, list):\n",
        "        return [strip_callables(v) for v in obj if not callable(v)]\n",
        "    if isinstance(obj, tuple):\n",
        "        return tuple(strip_callables(v) for v in obj if not callable(v))\n",
        "    return obj                     # leaf\n",
        "\n",
        "safe_history = strip_callables(best_overall[\"history\"])\n",
        "\n",
        "# Build the bundle for saving\n",
        "bundle = {\n",
        "    \"G_best\":  best_overall[\"G_best\"],\n",
        "    \"history\": safe_history,\n",
        "    \"l_list\":  l_list,\n",
        "    \"meta\": {\n",
        "        \"created\":      time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"val_loss\":     float(best_overall[\"val_loss\"]),\n",
        "        \"numpy_version\": np.__version__,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Dump to compressed pickle file\n",
        "out_path = pathlib.Path(\"g_and_history.pkl.gz\")\n",
        "with gzip.open(out_path, \"wb\") as f:\n",
        "    pickle.dump(bundle, f, protocol=5)\n",
        "\n",
        "print(\" Saved:\", out_path.resolve().as_posix())\n",
        "\n",
        "# Download\n",
        "files.download(str(out_path))"
      ],
      "metadata": {
        "id": "RynBudHxmpbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results Dataframe"
      ],
      "metadata": {
        "id": "Eui8to-xW4n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print results dataframe for further analysis."
      ],
      "metadata": {
        "id": "kSEHO8SMW9zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten results for DataFrame\n",
        "df_data = []\n",
        "for setting_name, results in all_results.items():\n",
        "    for run in range(repeats):\n",
        "        df_data.append({\n",
        "            \"Setting\": setting_name,\n",
        "            \"Run\": run + 1,\n",
        "            \"Train_Loss\": results[\"train_loss\"][run],\n",
        "            \"Val_Loss\": results[\"val_loss\"][run],\n",
        "            \"Test_Loss\": results[\"test_loss\"][run]\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(df_data)\n",
        "print(\"\\nResults DataFrame:\")\n",
        "print(results_df)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nSummary Statistics by Setting:\")\n",
        "print(\"=\" * 60)\n",
        "summary_stats = results_df.groupby('Setting')[['Train_Loss', 'Val_Loss', 'Test_Loss']].agg(['mean', 'std', 'min', 'max'])\n",
        "print(summary_stats)"
      ],
      "metadata": {
        "id": "Dxmid2-GW4FL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}